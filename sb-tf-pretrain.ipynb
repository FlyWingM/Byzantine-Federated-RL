{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "sb-tf1.15",
   "display_name": "sb-tf1.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating environment from the given name, wrapped in a DummyVecEnv.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\n--------------------------------------\n| % time spent exploring  | 75       |\n| episodes                | 100      |\n| mean 100 episode reward | 24.9     |\n| steps                   | 2463     |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 200      |\n| mean 100 episode reward | 81.5     |\n| steps                   | 10617    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 300      |\n| mean 100 episode reward | 118      |\n| steps                   | 22381    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 400      |\n| mean 100 episode reward | 108      |\n| steps                   | 33143    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 500      |\n| mean 100 episode reward | 104      |\n| steps                   | 43559    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 600      |\n| mean 100 episode reward | 117      |\n| steps                   | 55286    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 700      |\n| mean 100 episode reward | 314      |\n| steps                   | 86662    |\n--------------------------------------\nactions (5000, 1)\nobs (5000, 4)\nrewards (5000,)\nepisode_returns (10,)\nepisode_starts (5000,)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'actions': array([[0],\n        [0],\n        [1],\n        ...,\n        [0],\n        [0],\n        [1]]),\n 'obs': array([[-0.04735008, -0.01295144, -0.03380163, -0.00572591],\n        [-0.0476091 , -0.20757273, -0.03391615,  0.27610347],\n        [-0.05176056, -0.4021948 , -0.02839408,  0.55789924],\n        ...,\n        [ 0.471994  ,  0.17146152, -0.00464677, -0.24576311],\n        [ 0.47542325, -0.02359376, -0.00956203,  0.0454505 ],\n        [ 0.47495136, -0.2185773 , -0.00865302,  0.33510125]],\n       dtype=float32),\n 'rewards': array([1., 1., 1., ..., 1., 1., 1.]),\n 'episode_returns': array([500., 500., 500., 500., 500., 500., 500., 500., 500., 500.]),\n 'episode_starts': array([ True, False, False, ..., False, False, False])}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'generated_traj_data/expert_cartpole', n_timesteps=int(1e5), n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining using the saved trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "actions (5000, 1)\nobs (5000, 4)\nrewards (5000,)\nepisode_returns (10,)\nepisode_starts (5000,)\nTotal trajectories: -1\nTotal transitions: 5000\nAverage returns: 500.0\nStd for returns: 0.0\n"
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='generated_traj_data/expert_cartpole.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.init_dataloader(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.get_next_batch()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (s, a) -> (s+1, r)\n",
    "\n",
    "\n",
    "# TODO check assume not expert in IL\n",
    "(s0, s500) -> (a0, a500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['observations', 'actions', 'returns', 'avg_ret', 'std_ret', 'verbose', 'num_traj', 'num_transition', 'randomize', 'sequential_preprocessing', 'dataloader', 'train_loader', 'val_loader'])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating environment from the given name, wrapped in a DummyVecEnv.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n\nPretraining with Behavior Cloning...\n==== Training progress 10.00% ====\nEpoch 100\nTraining loss: 0.346077, Validation loss: 0.308957\n\n==== Training progress 20.00% ====\nEpoch 200\nTraining loss: 0.320585, Validation loss: 0.280822\n\n==== Training progress 30.00% ====\nEpoch 300\nTraining loss: 0.321907, Validation loss: 0.279914\n\n==== Training progress 40.00% ====\nEpoch 400\nTraining loss: 0.323388, Validation loss: 0.279585\n\n==== Training progress 50.00% ====\nEpoch 500\nTraining loss: 0.322113, Validation loss: 0.279030\n\n==== Training progress 60.00% ====\nEpoch 600\nTraining loss: 0.319191, Validation loss: 0.279283\n\n==== Training progress 70.00% ====\nEpoch 700\nTraining loss: 0.317897, Validation loss: 0.278338\n\n==== Training progress 80.00% ====\nEpoch 800\nTraining loss: 0.318741, Validation loss: 0.280931\n\n==== Training progress 90.00% ====\nEpoch 900\nTraining loss: 0.319314, Validation loss: 0.280736\n\n==== Training progress 100.00% ====\nEpoch 1000\nTraining loss: 0.314855, Validation loss: 0.281827\n\nPretraining done.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fbf81dee518>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "model = PPO2('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[500.]\n[500.]\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}