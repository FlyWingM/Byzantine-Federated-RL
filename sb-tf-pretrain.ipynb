{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "sb-tf1.15",
   "display_name": "sb-tf1.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nCreating environment from the given name, wrapped in a DummyVecEnv.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\nWARNING:tensorflow:From /home/flint/anaconda3/envs/sb-tf1.15/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\n--------------------------------------\n| % time spent exploring  | 79       |\n| episodes                | 100      |\n| mean 100 episode reward | 21.3     |\n| steps                   | 2111     |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 17       |\n| episodes                | 200      |\n| mean 100 episode reward | 63.1     |\n| steps                   | 8418     |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 300      |\n| mean 100 episode reward | 134      |\n| steps                   | 21868    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 400      |\n| mean 100 episode reward | 121      |\n| steps                   | 33942    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 500      |\n| mean 100 episode reward | 149      |\n| steps                   | 48836    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 600      |\n| mean 100 episode reward | 253      |\n| steps                   | 74112    |\n--------------------------------------\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 700      |\n| mean 100 episode reward | 215      |\n| steps                   | 95570    |\n--------------------------------------\nactions (1136, 1)\nobs (1136, 4)\nrewards (1136,)\nepisode_returns (10,)\nepisode_starts (1136,)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'actions': array([[0],\n        [0],\n        [0],\n        ...,\n        [1],\n        [0],\n        [1]]),\n 'obs': array([[ 2.24449742e-03, -5.75635722e-03, -3.96197960e-02,\n          1.58696324e-02],\n        [ 2.12937035e-03, -2.00288370e-01, -3.93024050e-02,\n          2.95793504e-01],\n        [-1.87639717e-03, -3.94828618e-01, -3.33865359e-02,\n          5.75826585e-01],\n        ...,\n        [ 2.33747888e+00,  1.32066762e+00, -7.42748603e-02,\n         -6.31480440e-02],\n        [ 2.36389208e+00,  1.51677155e+00, -7.55378231e-02,\n         -3.78309667e-01],\n        [ 2.39422750e+00,  1.32279909e+00, -8.31040144e-02,\n         -1.10367715e-01]], dtype=float32),\n 'rewards': array([1., 1., 1., ..., 1., 1., 1.]),\n 'episode_returns': array([117., 111., 116., 112., 114., 110., 114., 117., 115., 110.]),\n 'episode_starts': array([ True, False, False, ..., False, False, False])}"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.gail import generate_expert_traj\n",
    "\n",
    "model = DQN('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "      # Train a DQN agent for 1e5 timesteps and generate 10 trajectories\n",
    "      # data will be saved in a numpy archive named `expert_cartpole.npz`\n",
    "generate_expert_traj(model, 'expert_cartpole', n_timesteps=int(1e5), n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining using the saved trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "actions (1136, 1)\nobs (1136, 4)\nrewards (1136,)\nepisode_returns (10,)\nepisode_starts (1136,)\nTotal trajectories: -1\nTotal transitions: 1136\nAverage returns: 113.6\nStd for returns: 2.576819745345025\nCreating environment from the given name, wrapped in a DummyVecEnv.\nPretraining with Behavior Cloning...\n==== Training progress 10.00% ====\nEpoch 100\nTraining loss: 0.515627, Validation loss: 0.569196\n\n==== Training progress 20.00% ====\nEpoch 200\nTraining loss: 0.467308, Validation loss: 0.514591\n\n==== Training progress 30.00% ====\nEpoch 300\nTraining loss: 0.404181, Validation loss: 0.448127\n\n==== Training progress 40.00% ====\nEpoch 400\nTraining loss: 0.333797, Validation loss: 0.398611\n\n==== Training progress 50.00% ====\nEpoch 500\nTraining loss: 0.278037, Validation loss: 0.357821\n\n==== Training progress 60.00% ====\nEpoch 600\nTraining loss: 0.248703, Validation loss: 0.311816\n\n==== Training progress 70.00% ====\nEpoch 700\nTraining loss: 0.236393, Validation loss: 0.294334\n\n==== Training progress 80.00% ====\nEpoch 800\nTraining loss: 0.216720, Validation loss: 0.264402\n\n==== Training progress 90.00% ====\nEpoch 900\nTraining loss: 0.209176, Validation loss: 0.245762\n\n==== Training progress 100.00% ====\nEpoch 1000\nTraining loss: 0.191244, Validation loss: 0.240749\n\nPretraining done.\n[113.]\n[116.]\n[119.]\n[110.]\n[13.]\n[117.]\n[113.]\n[119.]\n[129.]\n"
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='expert_cartpole.npz',\n",
    "                        traj_limitation=-1, batch_size=128)\n",
    "\n",
    "model = PPO2('MlpPolicy', 'CartPole-v1', verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}