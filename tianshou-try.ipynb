{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitbyzantinerlcondae8fdb4d46cb74bcbae215deda368d289",
   "display_name": "Python 3.6.10 64-bit ('ByzantineRL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tianshou as ts\n",
    "\n",
    "\n",
    "ENV_N = 3\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "envs = [gym.make(ENV_NAME) for _ in range(ENV_N)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initiate multiple envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = [gym.make(ENV_NAME) for _ in range(ENV_N)] # each individual agent per env\n",
    "test_envs = [gym.make(ENV_NAME) for _ in range(ENV_N)] ## TODO: think? do we need one test env for all policies or each policies need one test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define policy networks and optimization criteria\n",
    "\n",
    "import torch, numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*[\n",
    "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        ])\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [Net(state_shape, action_shape) for _ in range(ENV_N)]\n",
    "\n",
    "policies = [ts.policy.DQNPolicy(nets[i],  # net\n",
    "                                torch.optim.Adam(nets[i].parameters(), lr=1e-3),  # optim\n",
    "                                discount_factor=0.9, \n",
    "                                estimation_step=3, \n",
    "                                target_update_freq=320) for i in range(ENV_N)]\n",
    "assert len(policies) == len(train_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collector\n",
    "\n",
    "The collector is a key concept in Tianshou. It allows the policy to interact with different types of environments conveniently. In each step, the collector will let the policy perform (at least) a specified number of steps or episodes and store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collectors = [ts.data.Collector(policies[i], \n",
    "                                      train_envs[i], \n",
    "                                      ts.data.ReplayBuffer(size=1000)) for i in range(ENV_N)]\n",
    "test_collectors = [ts.data.Collector(policies[i], test_envs[i]) for i in range(ENV_N)] ## TODO: think? do we need one test env for all policies or each policies need one test env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch #1:   2%|2         | 20/1000 [00:00<00:10, 94.15it/s, len=0.00, loss=0.533388, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=13.83, v/st=1951.27]Training policy 0 on env 0\nEpoch #1: 1001it [00:20, 49.39it/s, len=0.00, loss=0.015264, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=11.47, v/st=1910.05]\nEpoch #2:   2%|2         | 20/1000 [00:00<00:09, 105.23it/s, len=0.00, loss=0.014602, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=11.31, v/st=1899.52]Epoch #1: test_reward: 144.440000, best_reward: 144.440000 in #1\nEpoch #2: 1001it [00:09, 105.44it/s, len=0.00, loss=0.022661, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.36, v/st=1966.98]\nEpoch #1:   2%|2         | 22/1000 [00:00<00:08, 111.84it/s, len=0.00, loss=3.582766, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=191.03, v/st=2012.80]Epoch #2: test_reward: 173.010000, best_reward: 173.010000 in #2\nFinished training! Use 41.99s\nTraining policy 1 on env 1\nEpoch #1: 1001it [00:09, 107.24it/s, len=186.00, loss=0.566913, n/ep=1.00, n/st=10.00, rew=186.00, v/ep=13.65, v/st=1953.71]\nEpoch #2:   2%|2         | 20/1000 [00:00<00:09, 107.53it/s, len=0.00, loss=0.512030, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=14.24, v/st=1946.46]Epoch #1: test_reward: 148.800000, best_reward: 148.800000 in #1\nEpoch #2: 1001it [00:09, 105.27it/s, len=0.00, loss=0.043462, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=9.64, v/st=1995.96]\nEpoch #1:   2%|2         | 21/1000 [00:00<00:09, 107.33it/s, len=12.00, loss=3.661986, n/ep=1.00, n/st=10.00, rew=12.00, v/ep=190.16, v/st=1920.20]Epoch #2: test_reward: 236.750000, best_reward: 236.750000 in #2\nFinished training! Use 33.46s\nTraining policy 2 on env 2\nEpoch #1: 1001it [00:09, 106.91it/s, len=0.00, loss=0.279906, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=9.52, v/st=1946.93]\nEpoch #2:   2%|2         | 21/1000 [00:00<00:09, 106.49it/s, len=0.00, loss=0.265441, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=9.76, v/st=1956.23]Epoch #1: test_reward: 161.610000, best_reward: 161.610000 in #1\nEpoch #2: 1001it [00:19, 50.89it/s, len=0.00, loss=0.088912, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=14.49, v/st=1981.32]\nEpoch #2: test_reward: 153.410000, best_reward: 161.610000 in #1\nFinished training! Use 41.23s\n"
    }
   ],
   "source": [
    "results = [[] for _ in range(ENV_N)]\n",
    "for j in range(ENV_N):\n",
    "    print(f\"Training policy {j} on env {j}\")\n",
    "    results[j] = ts.trainer.offpolicy_trainer(\n",
    "        policies[j], train_collectors[j], test_collectors[j],\n",
    "        max_epoch=2, step_per_epoch=1000, collect_per_step=10,\n",
    "        episode_per_test=100, batch_size=64,\n",
    "        train_fn=lambda e: policies[j].set_eps(0.1),\n",
    "        test_fn=lambda e: policies[j].set_eps(0.05),\n",
    "        stop_fn=lambda x: x >= env.spec.reward_threshold,\n",
    "        writer=None)\n",
    "    print(f'Finished training! Use {results[j][\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reward from above training\n",
    "- 173\n",
    "- 236\n",
    "- 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive data from reply buffer after training\n",
    "train_collector_0 = train_collectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Batch(\n    policy: Batch(\n                _state: Batch(),\n            ),\n    obs_next: array([[-0.0366928 , -0.34702743, -0.00845681,  0.33000577]]),\n    act: array([0]),\n    obs: array([[-0.0366928 , -0.34702743, -0.00845681,  0.33000577]]),\n    rew: array([1.]),\n    done: array([False]),\n    state: Batch(),\n    info: Batch(),\n)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_collector_0.data.cat(train_collectors[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('log/dqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train_step': 11940,\n 'train_episode': 584.0,\n 'train_time/collector': '6.14s',\n 'train_time/model': '4.87s',\n 'train_speed': '1084.19 step/s',\n 'test_step': 200658,\n 'test_episode': 1200.0,\n 'test_time': '76.90s',\n 'test_speed': '2609.21 step/s',\n 'best_reward': 196.88,\n 'duration': '87.92s'}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn-best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}