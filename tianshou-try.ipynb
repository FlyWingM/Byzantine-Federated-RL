{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitbyzantinerlcondae8fdb4d46cb74bcbae215deda368d289",
   "display_name": "Python 3.6.10 64-bit ('ByzantineRL': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tianshou as ts\n",
    "\n",
    "\n",
    "ENV_N = 10\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "envs = [gym.make(ENV_NAME) for _ in range(ENV_N)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initiate multiple envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = [gym.make(ENV_NAME) for _ in range(ENV_N)] # each individual agent per env\n",
    "test_envs = [gym.make(ENV_NAME) for _ in range(ENV_N)] ## TODO: think? do we need one test env for all policies or each policies need one test env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define policy networks and optimization criteria\n",
    "\n",
    "import torch, numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*[\n",
    "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, np.prod(action_shape))\n",
    "        ])\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state\n",
    "\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape, action_shape)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [Net(state_shape, action_shape) for _ in range(ENV_N)]\n",
    "\n",
    "# policies = [ts.policy.DQNPolicy(nets[i],  # net\n",
    "#                                 torch.optim.Adam(nets[i].parameters(), lr=1e-3),  # optim\n",
    "#                                 discount_factor=0.9, \n",
    "#                                 estimation_step=3, \n",
    "#                                 target_update_freq=320) for i in range(ENV_N)]\n",
    "policies = [ts.policy.DQNPolicy(nets[i],  # net\n",
    "                                torch.optim.Adam(nets[i].parameters(), lr=1e-3),  # optim\n",
    "                                discount_factor=0.9, \n",
    "                                estimation_step=3, \n",
    "                                target_update_freq=320) for i in range(ENV_N)]\n",
    "\n",
    "assert len(policies) == len(train_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collector\n",
    "\n",
    "The collector is a key concept in Tianshou. It allows the policy to interact with different types of environments conveniently. In each step, the collector will let the policy perform (at least) a specified number of steps or episodes and store the data in a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collectors = [ts.data.Collector(policies[i], \n",
    "                                      train_envs[i], \n",
    "                                      ts.data.ReplayBuffer(size=1000)) for i in range(ENV_N)]\n",
    "test_collectors = [ts.data.Collector(policies[i], test_envs[i]) for i in range(ENV_N)] ## TODO: think? do we need one test env for all policies or each policies need one test env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch #1:   2%|2         | 23/1000 [00:00<00:08, 109.56it/s, len=11.00, loss=3.230363, n/ep=1.00, n/st=10.00, rew=11.00, v/ep=199.54, v/st=2017.19]Training policy 0 on env 0\nEpoch #1: 1001it [00:09, 110.13it/s, len=0.00, loss=0.282627, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=17.82, v/st=2052.60]\nEpoch #2:   2%|2         | 20/1000 [00:00<00:09, 106.61it/s, len=0.00, loss=0.326034, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=21.72, v/st=2023.19]Epoch #1: test_reward: 33.320000, best_reward: 33.320000 in #1\nEpoch #2: 1001it [00:09, 109.93it/s, len=164.00, loss=0.046963, n/ep=1.00, n/st=10.00, rew=164.00, v/ep=12.30, v/st=2055.77]\nEpoch #1:   2%|2         | 22/1000 [00:00<00:08, 115.56it/s, len=10.00, loss=3.614060, n/ep=1.00, n/st=10.00, rew=10.00, v/ep=208.41, v/st=1990.51]Epoch #2: test_reward: 147.650000, best_reward: 147.650000 in #2\nFinished training! Use 24.55s\nTraining policy 1 on env 1\nEpoch #1: 1001it [00:08, 115.18it/s, len=0.00, loss=0.155781, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=6.35, v/st=2044.20]\nEpoch #2:   2%|2         | 23/1000 [00:00<00:08, 116.21it/s, len=0.00, loss=0.143432, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=6.20, v/st=2068.04]Epoch #1: test_reward: 341.380000, best_reward: 341.380000 in #1\nEpoch #2: 1001it [00:17, 55.90it/s, len=0.00, loss=0.150728, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=6.41, v/st=2071.67]\nEpoch #1:   2%|2         | 23/1000 [00:00<00:08, 109.22it/s, len=10.00, loss=3.805049, n/ep=1.00, n/st=10.00, rew=10.00, v/ep=204.94, v/st=2050.48]Epoch #2: test_reward: 439.880000, best_reward: 439.880000 in #2\nFinished training! Use 54.21s\nTraining policy 2 on env 2\nEpoch #1: 1001it [00:08, 114.54it/s, len=0.00, loss=0.148254, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.61, v/st=2092.91]\nEpoch #2:   2%|2         | 23/1000 [00:00<00:08, 114.38it/s, len=0.00, loss=0.161771, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.60, v/st=2099.77]Epoch #1: test_reward: 198.470000, best_reward: 198.470000 in #1\nEpoch #2: 1001it [00:08, 113.49it/s, len=0.00, loss=0.127623, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.16, v/st=2098.28]\nEpoch #1:   2%|2         | 23/1000 [00:00<00:08, 118.53it/s, len=9.00, loss=3.231714, n/ep=1.00, n/st=10.00, rew=9.00, v/ep=206.25, v/st=2062.55]Epoch #2: test_reward: 246.640000, best_reward: 246.640000 in #2\nFinished training! Use 33.12s\nTraining policy 3 on env 3\nEpoch #1: 1001it [00:08, 114.94it/s, len=0.00, loss=0.140326, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=8.09, v/st=2064.11]\nEpoch #2:   2%|2         | 23/1000 [00:00<00:08, 116.74it/s, len=0.00, loss=0.156518, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=8.29, v/st=2071.91]Epoch #1: test_reward: 212.600000, best_reward: 212.600000 in #1\nEpoch #2: 1001it [00:08, 114.29it/s, len=0.00, loss=0.233253, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=16.42, v/st=2116.61]\nEpoch #1:   2%|2         | 23/1000 [00:00<00:08, 119.35it/s, len=9.00, loss=3.319625, n/ep=1.00, n/st=10.00, rew=9.00, v/ep=197.37, v/st=2070.51]Epoch #2: test_reward: 142.540000, best_reward: 212.600000 in #1\nFinished training! Use 29.77s\nTraining policy 4 on env 4\nEpoch #1: 1001it [00:08, 115.13it/s, len=0.00, loss=0.189744, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=9.75, v/st=2051.41]\nEpoch #2:   2%|2         | 23/1000 [00:00<00:08, 115.92it/s, len=0.00, loss=0.211299, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.02, v/st=2042.42]Epoch #1: test_reward: 222.930000, best_reward: 222.930000 in #1\nEpoch #2: 1001it [00:08, 113.13it/s, len=0.00, loss=0.092911, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=14.23, v/st=2047.44]\nEpoch #1:   2%|2         | 23/1000 [00:00<00:08, 118.12it/s, len=0.00, loss=3.359475, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=208.37, v/st=2093.85]Epoch #2: test_reward: 142.450000, best_reward: 222.930000 in #1\nFinished training! Use 30.13s\nTraining policy 5 on env 5\nEpoch #1: 1001it [00:08, 115.63it/s, len=0.00, loss=0.226424, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.09, v/st=2100.44]\nEpoch #2:   2%|2         | 24/1000 [00:00<00:08, 118.68it/s, len=0.00, loss=0.249255, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.21, v/st=2134.67]Epoch #1: test_reward: 185.310000, best_reward: 185.310000 in #1\nEpoch #2: 1001it [00:09, 110.04it/s, len=0.00, loss=0.170732, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.94, v/st=1990.80]\nEpoch #1:   2%|2         | 21/1000 [00:00<00:09, 103.43it/s, len=10.00, loss=3.526418, n/ep=1.00, n/st=10.00, rew=10.00, v/ep=183.74, v/st=1865.27]Epoch #2: test_reward: 164.220000, best_reward: 185.310000 in #1\nFinished training! Use 30.27s\nTraining policy 6 on env 6\nEpoch #1: 1001it [00:09, 107.98it/s, len=0.00, loss=0.183835, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.59, v/st=2005.29]\nEpoch #2:   2%|2         | 21/1000 [00:00<00:09, 106.74it/s, len=0.00, loss=0.203051, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.62, v/st=2019.03]Epoch #1: test_reward: 238.040000, best_reward: 238.040000 in #1\nEpoch #2: 1001it [00:09, 106.71it/s, len=0.00, loss=0.062204, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=15.70, v/st=1992.45]\nEpoch #1:   2%|2         | 21/1000 [00:00<00:08, 111.47it/s, len=9.00, loss=4.017612, n/ep=1.00, n/st=10.00, rew=9.00, v/ep=193.10, v/st=1913.46]Epoch #2: test_reward: 132.590000, best_reward: 238.040000 in #1\nFinished training! Use 32.83s\nTraining policy 7 on env 7\nEpoch #1: 1001it [00:09, 107.42it/s, len=0.00, loss=0.122204, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=6.33, v/st=1970.68]\nEpoch #2:   2%|2         | 20/1000 [00:00<00:10, 95.14it/s, len=0.00, loss=0.129081, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=6.33, v/st=1956.48]Epoch #1: test_reward: 203.040000, best_reward: 203.040000 in #1\nEpoch #2: 1001it [00:20, 47.67it/s, len=0.00, loss=0.128051, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=7.59, v/st=1990.54]\nEpoch #1:   2%|2         | 23/1000 [00:00<00:08, 115.35it/s, len=10.00, loss=3.491397, n/ep=1.00, n/st=10.00, rew=10.00, v/ep=209.23, v/st=2079.29]Epoch #2: test_reward: 213.880000, best_reward: 213.880000 in #2\nFinished training! Use 46.65s\nTraining policy 8 on env 8\nEpoch #1: 1001it [00:09, 106.90it/s, len=0.00, loss=0.477300, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=13.24, v/st=1970.17]\nEpoch #2:   2%|2         | 21/1000 [00:00<00:09, 108.06it/s, len=0.00, loss=0.474055, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=11.50, v/st=2006.35]Epoch #1: test_reward: 193.790000, best_reward: 193.790000 in #1\nEpoch #2: 1001it [00:09, 106.24it/s, len=0.00, loss=0.279876, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=15.65, v/st=1924.45]\nEpoch #1:   2%|2         | 20/1000 [00:00<00:09, 98.91it/s, len=11.00, loss=3.253698, n/ep=1.00, n/st=10.00, rew=11.00, v/ep=184.98, v/st=1849.80]Epoch #2: test_reward: 154.530000, best_reward: 193.790000 in #1\nFinished training! Use 32.03s\nTraining policy 9 on env 9\nEpoch #1: 1001it [00:09, 106.79it/s, len=0.00, loss=0.264222, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=7.80, v/st=1960.07]\nEpoch #2:   2%|2         | 20/1000 [00:00<00:09, 99.46it/s, len=0.00, loss=0.251407, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=7.63, v/st=1937.27]Epoch #1: test_reward: 460.640000, best_reward: 460.640000 in #1\nEpoch #2: 1001it [00:09, 107.14it/s, len=0.00, loss=0.204375, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.09, v/st=2003.19]\nEpoch #2: test_reward: 214.410000, best_reward: 460.640000 in #1\nFinished training! Use 44.28s\n"
    }
   ],
   "source": [
    "results = [[] for _ in range(ENV_N)]\n",
    "for j in range(ENV_N):\n",
    "    print(f\"Training policy {j} on env {j}\")\n",
    "    results[j] = ts.trainer.offpolicy_trainer(\n",
    "        policies[j], train_collectors[j], test_collectors[j],\n",
    "        max_epoch=2, step_per_epoch=1000, collect_per_step=10,\n",
    "        episode_per_test=100, batch_size=64,\n",
    "        train_fn=lambda e: policies[j].set_eps(0.1),\n",
    "        test_fn=lambda e: policies[j].set_eps(0.05),\n",
    "        stop_fn=lambda x: x >= env.spec.reward_threshold,\n",
    "        writer=None)\n",
    "    print(f'Finished training! Use {results[j][\"duration\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initiate a new policy which won't interact with env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = Net(state_shape, action_shape)\n",
    "\n",
    "global_policy = ts.policy.DQNPolicy(global_net,  # net\n",
    "                                torch.optim.Adam(global_net.parameters(), lr=1e-3),  # optim\n",
    "                                discount_factor=0.9, \n",
    "                                estimation_step=3, \n",
    "                                target_update_freq=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n(1000,)\n"
    }
   ],
   "source": [
    "# take collector 0's buffer for training the new policy\n",
    "# num_G_step = 1000\n",
    "# batch_size = 128\n",
    "for j in range(ENV_N):\n",
    "    batch, indice = train_collectors[j].buffer.sample(0) # batch_size = 0, take the whole batch from reply buffer\n",
    "    print(batch.rew.shape)\n",
    "    batch = global_policy.process_fn(batch, train_collectors[j].buffer, indice) # processing \n",
    "    global_policy.learn(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now evaluate the global policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env_global = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector_global = ts.data.Collector(global_policy, test_env_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_collector_global.collect(n_episode=5, render=1 / 35 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'n/ep': 5.0, 'n/st': 190, 'v/st': 33.27685260854186, 'v/ep': 0.8757066475932068, 'rew': 38.0, 'len': 38.0}\n"
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ENV_N   |      reply buff size      |  rewards of local policies | reward of test policy (global) |\n",
    "|:----------:|:-------------:|:------:|:------:|\n",
    "| 3 |  1000 |  240, 213, 196 | 27.5 |\n",
    "| 10 |    1000   | 147, 439, 246, 212, 222, 185, 238, 213, 193, 460  | 38 |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('log/dqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'train_step': 11940,\n 'train_episode': 584.0,\n 'train_time/collector': '6.14s',\n 'train_time/model': '4.87s',\n 'train_speed': '1084.19 step/s',\n 'test_step': 200658,\n 'test_episode': 1200.0,\n 'test_time': '76.90s',\n 'test_speed': '2609.21 step/s',\n 'best_reward': 196.88,\n 'duration': '87.92s'}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn-best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Epoch #1: 1001it [00:09, 107.26it/s, len=0.00, loss=0.039329, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=11.86, v/st=1991.73]\nEpoch #2:   2%|2         | 22/1000 [00:00<00:08, 112.93it/s, len=0.00, loss=0.048284, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=10.14, v/st=1999.42]Epoch #1: test_reward: 152.870000, best_reward: 152.870000 in #1\nEpoch #2:  72%|#######1  | 718/1000 [00:06<00:02, 105.88it/s, len=0.00, loss=0.023279, n/ep=0.00, n/st=10.00, rew=0.00, v/ep=9.90, v/st=1951.06]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0dec5d4eca5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtest_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mstop_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         writer=None)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ByzantineRL/lib/python3.6/site-packages/tianshou/trainer/offpolicy.py\u001b[0m in \u001b[0;36moffpolicy_trainer\u001b[0;34m(policy, train_collector, test_collector, max_epoch, step_per_epoch, collect_per_step, episode_per_test, batch_size, update_per_step, train_fn, test_fn, stop_fn, save_fn, log_fn, writer, log_interval, verbose, test_in_train)\u001b[0m\n\u001b[1;32m    107\u001b[0m                         result['n/st'] // collect_per_step, t.total - t.n)):\n\u001b[1;32m    108\u001b[0m                     \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{result[k]:.2f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ByzantineRL/lib/python3.6/site-packages/tianshou/data/collector.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \"\"\"\n\u001b[1;32m    392\u001b[0m         \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ByzantineRL/lib/python3.6/site-packages/tianshou/policy/modelfree/dqn.py\u001b[0m in \u001b[0;36mprocess_fn\u001b[0;34m(self, batch, buffer, indice)\u001b[0m\n\u001b[1;32m     91\u001b[0m         batch = self.compute_nstep_return(\n\u001b[1;32m     92\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             self._gamma, self._n_step, self._rew_norm)\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ByzantineRL/lib/python3.6/site-packages/tianshou/policy/base.py\u001b[0m in \u001b[0;36mcompute_nstep_return\u001b[0;34m(batch, buffer, indice, target_q_fn, gamma, n_step, rew_norm)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindice\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbuf_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindice\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbuf_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "ts.trainer.offpolicy_trainer(\n",
    "        policies[j], train_collectors[0], test_collectors[j],\n",
    "        max_epoch=2, step_per_epoch=1000, collect_per_step=10,\n",
    "        episode_per_test=100, batch_size=64,\n",
    "        train_fn=lambda e: policies[j].set_eps(0.1),\n",
    "        test_fn=lambda e: policies[j].set_eps(0.05),\n",
    "        stop_fn=lambda x: x >= env.spec.reward_threshold,\n",
    "        writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}