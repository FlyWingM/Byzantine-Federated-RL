{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitsb3torch16conda1afb2d79dad84afc9514abe2961e309e",
   "display_name": "Python 3.6.10 64-bit ('sb3-torch1.6': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous actions\n",
    "# env_id = \"LunarLanderContinuous-v2\"\n",
    "\n",
    "# discrete actions\n",
    "env_id = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using cuda device\nCreating environment from the given name, wrapped in a DummyVecEnv.\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 23.7     |\n|    ep_rew_mean     | 23.7     |\n| time/              |          |\n|    fps             | 1241     |\n|    iterations      | 1        |\n|    time_elapsed    | 1        |\n|    total_timesteps | 2048     |\n---------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 26.2        |\n|    ep_rew_mean          | 26.2        |\n| time/                   |             |\n|    fps                  | 895         |\n|    iterations           | 2           |\n|    time_elapsed         | 4           |\n|    total_timesteps      | 4096        |\n| train/                  |             |\n|    approx_kl            | 0.006888212 |\n|    clip_fraction        | 0.0938      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.686      |\n|    explained_variance   | -216        |\n|    learning_rate        | 0.0003      |\n|    loss                 | 7.83        |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0157     |\n|    value_loss           | 57.4        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 34.6        |\n|    ep_rew_mean          | 34.6        |\n| time/                   |             |\n|    fps                  | 829         |\n|    iterations           | 3           |\n|    time_elapsed         | 7           |\n|    total_timesteps      | 6144        |\n| train/                  |             |\n|    approx_kl            | 0.013550044 |\n|    clip_fraction        | 0.0938      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.668      |\n|    explained_variance   | -70.1       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 14.4        |\n|    n_updates            | 20          |\n|    policy_gradient_loss | -0.0199     |\n|    value_loss           | 40          |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 49.1       |\n|    ep_rew_mean          | 49.1       |\n| time/                   |            |\n|    fps                  | 796        |\n|    iterations           | 4          |\n|    time_elapsed         | 10         |\n|    total_timesteps      | 8192       |\n| train/                  |            |\n|    approx_kl            | 0.01242266 |\n|    clip_fraction        | 0.156      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.637     |\n|    explained_variance   | -5.18      |\n|    learning_rate        | 0.0003     |\n|    loss                 | 23.6       |\n|    n_updates            | 30         |\n|    policy_gradient_loss | -0.0228    |\n|    value_loss           | 56.2       |\n----------------------------------------\nEval num_timesteps=10000, episode_reward=406.80 +/- 114.71\nEpisode length: 406.80 +/- 114.71\nNew best mean reward!\n------------------------------------------\n| eval/                   |              |\n|    mean_ep_length       | 407          |\n|    mean_reward          | 407          |\n| rollout/                |              |\n|    ep_len_mean          | 63.4         |\n|    ep_rew_mean          | 63.4         |\n| time/                   |              |\n|    fps                  | 706          |\n|    iterations           | 5            |\n|    time_elapsed         | 14           |\n|    total_timesteps      | 10240        |\n| train/                  |              |\n|    approx_kl            | 0.0076474845 |\n|    clip_fraction        | 0.0938       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.614       |\n|    explained_variance   | -3.7         |\n|    learning_rate        | 0.0003       |\n|    loss                 | 25.8         |\n|    n_updates            | 40           |\n|    policy_gradient_loss | -0.0133      |\n|    value_loss           | 69.4         |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 78.4         |\n|    ep_rew_mean          | 78.4         |\n| time/                   |              |\n|    fps                  | 704          |\n|    iterations           | 6            |\n|    time_elapsed         | 17           |\n|    total_timesteps      | 12288        |\n| train/                  |              |\n|    approx_kl            | 0.0101180095 |\n|    clip_fraction        | 0.141        |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.588       |\n|    explained_variance   | -0.357       |\n|    learning_rate        | 0.0003       |\n|    loss                 | 35           |\n|    n_updates            | 50           |\n|    policy_gradient_loss | -0.0127      |\n|    value_loss           | 67           |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 94           |\n|    ep_rew_mean          | 94           |\n| time/                   |              |\n|    fps                  | 703          |\n|    iterations           | 7            |\n|    time_elapsed         | 20           |\n|    total_timesteps      | 14336        |\n| train/                  |              |\n|    approx_kl            | 0.0021434487 |\n|    clip_fraction        | 0.0625       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.58        |\n|    explained_variance   | -5.88        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 22.1         |\n|    n_updates            | 60           |\n|    policy_gradient_loss | -0.00753     |\n|    value_loss           | 68           |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 111          |\n|    ep_rew_mean          | 111          |\n| time/                   |              |\n|    fps                  | 702          |\n|    iterations           | 8            |\n|    time_elapsed         | 23           |\n|    total_timesteps      | 16384        |\n| train/                  |              |\n|    approx_kl            | 0.0037376978 |\n|    clip_fraction        | 0.0312       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.57        |\n|    explained_variance   | -0.523       |\n|    learning_rate        | 0.0003       |\n|    loss                 | 6.93         |\n|    n_updates            | 70           |\n|    policy_gradient_loss | -0.00307     |\n|    value_loss           | 51.2         |\n------------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 127          |\n|    ep_rew_mean          | 127          |\n| time/                   |              |\n|    fps                  | 701          |\n|    iterations           | 9            |\n|    time_elapsed         | 26           |\n|    total_timesteps      | 18432        |\n| train/                  |              |\n|    approx_kl            | 0.0058772024 |\n|    clip_fraction        | 0            |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.556       |\n|    explained_variance   | 0.753        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 3.44         |\n|    n_updates            | 80           |\n|    policy_gradient_loss | -0.00382     |\n|    value_loss           | 37           |\n------------------------------------------\nEval num_timesteps=20000, episode_reward=464.00 +/- 72.00\nEpisode length: 464.00 +/- 72.00\nNew best mean reward!\n------------------------------------------\n| eval/                   |              |\n|    mean_ep_length       | 464          |\n|    mean_reward          | 464          |\n| rollout/                |              |\n|    ep_len_mean          | 140          |\n|    ep_rew_mean          | 140          |\n| time/                   |              |\n|    fps                  | 669          |\n|    iterations           | 10           |\n|    time_elapsed         | 30           |\n|    total_timesteps      | 20480        |\n| train/                  |              |\n|    approx_kl            | 0.0058471616 |\n|    clip_fraction        | 0.0781       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.547       |\n|    explained_variance   | 0.801        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 5.69         |\n|    n_updates            | 90           |\n|    policy_gradient_loss | -0.00508     |\n|    value_loss           | 33.3         |\n------------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 155         |\n|    ep_rew_mean          | 155         |\n| time/                   |             |\n|    fps                  | 671         |\n|    iterations           | 11          |\n|    time_elapsed         | 33          |\n|    total_timesteps      | 22528       |\n| train/                  |             |\n|    approx_kl            | 0.007230124 |\n|    clip_fraction        | 0.0312      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.542      |\n|    explained_variance   | 0.863       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 1.23        |\n|    n_updates            | 100         |\n|    policy_gradient_loss | -0.00571    |\n|    value_loss           | 24.7        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 169         |\n|    ep_rew_mean          | 169         |\n| time/                   |             |\n|    fps                  | 673         |\n|    iterations           | 12          |\n|    time_elapsed         | 36          |\n|    total_timesteps      | 24576       |\n| train/                  |             |\n|    approx_kl            | 0.010596907 |\n|    clip_fraction        | 0.0312      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.546      |\n|    explained_variance   | 0.736       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 4.05        |\n|    n_updates            | 110         |\n|    policy_gradient_loss | -0.00538    |\n|    value_loss           | 27.6        |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 182         |\n|    ep_rew_mean          | 182         |\n| time/                   |             |\n|    fps                  | 675         |\n|    iterations           | 13          |\n|    time_elapsed         | 39          |\n|    total_timesteps      | 26624       |\n| train/                  |             |\n|    approx_kl            | 0.008115706 |\n|    clip_fraction        | 0.172       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.556      |\n|    explained_variance   | 0.831       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 4           |\n|    n_updates            | 120         |\n|    policy_gradient_loss | -0.00768    |\n|    value_loss           | 33.1        |\n-----------------------------------------\n------------------------------------------\n| rollout/                |              |\n|    ep_len_mean          | 200          |\n|    ep_rew_mean          | 200          |\n| time/                   |              |\n|    fps                  | 677          |\n|    iterations           | 14           |\n|    time_elapsed         | 42           |\n|    total_timesteps      | 28672        |\n| train/                  |              |\n|    approx_kl            | 0.0063132457 |\n|    clip_fraction        | 0.0469       |\n|    clip_range           | 0.2          |\n|    entropy_loss         | -0.548       |\n|    explained_variance   | 0.884        |\n|    learning_rate        | 0.0003       |\n|    loss                 | 4.57         |\n|    n_updates            | 130          |\n|    policy_gradient_loss | -0.00923     |\n|    value_loss           | 23           |\n------------------------------------------\nEval num_timesteps=30000, episode_reward=500.00 +/- 0.00\nEpisode length: 500.00 +/- 0.00\nNew best mean reward!\n-----------------------------------------\n| eval/                   |             |\n|    mean_ep_length       | 500         |\n|    mean_reward          | 500         |\n| rollout/                |             |\n|    ep_len_mean          | 215         |\n|    ep_rew_mean          | 215         |\n| time/                   |             |\n|    fps                  | 657         |\n|    iterations           | 15          |\n|    time_elapsed         | 46          |\n|    total_timesteps      | 30720       |\n| train/                  |             |\n|    approx_kl            | 0.007968318 |\n|    clip_fraction        | 0.0781      |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -0.554      |\n|    explained_variance   | 0.589       |\n|    learning_rate        | 0.0003      |\n|    loss                 | 14.4        |\n|    n_updates            | 140         |\n|    policy_gradient_loss | -0.0096     |\n|    value_loss           | 60.2        |\n-----------------------------------------\n"
    }
   ],
   "source": [
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
    "ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n",
    "ppo_expert.save(\"ppo_expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean reward = 500.0 +/- 0.0\n"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create student policy\n",
    "### it may be diffrent algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using cuda device\nCreating environment from the given name, wrapped in a DummyVecEnv.\n"
    }
   ],
   "source": [
    "ppo_student = PPO('MlpPolicy', env_id, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only valid for continuous actions\n",
    "# sac_student = SAC('MlpPolicy', env_id, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of samples from expert to be used as \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = int(4e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 40000/40000 [00:23<00:00, 1684.34it/s]\n"
    }
   ],
   "source": [
    "if isinstance(env.action_space, gym.spaces.Box):\n",
    "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "  expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
    "\n",
    "else:\n",
    "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "  expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "    expert_observations[i] = obs\n",
    "    expert_actions[i] = action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"expert_data\",\n",
    "    expert_actions=expert_actions,\n",
    "    expert_observations=expert_observations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now reuse the generated expert data to train the student policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "train_size = int(0.8 * len(expert_dataset))\n",
    "\n",
    "test_size = len(expert_dataset) - train_size\n",
    "\n",
    "train_expert_dataset, test_expert_dataset = random_split(\n",
    "    expert_dataset, [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test_expert_dataset:  8000\ntrain_expert_dataset:  32000\n"
    }
   ],
   "source": [
    "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
    "print(\"train_expert_dataset: \", len(train_expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "      criterion = nn.MSELoss()\n",
    "    else:\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "              # A2C/PPO policy outputs actions, values, log_prob\n",
    "              # SAC/TD3 policy outputs actions only\n",
    "              if isinstance(student, (A2C, PPO)):\n",
    "                action, _, _ = model(data)\n",
    "              else:\n",
    "                # SAC/TD3:\n",
    "                action = model(data)\n",
    "              action_prediction = action.double()\n",
    "            else:\n",
    "              # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "              latent_pi, _, _ = model._get_latent(data)\n",
    "              logits = model.action_net(latent_pi)\n",
    "              action_prediction = logits\n",
    "              target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                  # A2C/PPO policy outputs actions, values, log_prob\n",
    "                  # SAC/TD3 policy outputs actions only\n",
    "                  if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                  else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                  action_prediction = action.double()\n",
    "                else:\n",
    "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                  latent_pi, _, _ = model._get_latent(data)\n",
    "                  logits = model.action_net(latent_pi)\n",
    "                  action_prediction = logits\n",
    "                  target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # and testing\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
    "    )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    ppo_student.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate the agent before pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean reward = 8.9 +/- 0.5385164807134504\n"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_student, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.693642\nTrain Epoch: 1 [6400/32000 (20%)]\tLoss: 0.374572\nTrain Epoch: 1 [12800/32000 (40%)]\tLoss: 0.321954\nTrain Epoch: 1 [19200/32000 (60%)]\tLoss: 0.325045\nTrain Epoch: 1 [25600/32000 (80%)]\tLoss: 0.161926\nTest set: Average loss: 0.0000\nTrain Epoch: 2 [0/32000 (0%)]\tLoss: 0.210440\nTrain Epoch: 2 [6400/32000 (20%)]\tLoss: 0.245195\nTrain Epoch: 2 [12800/32000 (40%)]\tLoss: 0.218046\nTrain Epoch: 2 [19200/32000 (60%)]\tLoss: 0.308945\nTrain Epoch: 2 [25600/32000 (80%)]\tLoss: 0.227988\nTest set: Average loss: 0.0000\nTrain Epoch: 3 [0/32000 (0%)]\tLoss: 0.181285\nTrain Epoch: 3 [6400/32000 (20%)]\tLoss: 0.268268\nTrain Epoch: 3 [12800/32000 (40%)]\tLoss: 0.154440\nTrain Epoch: 3 [19200/32000 (60%)]\tLoss: 0.122000\nTrain Epoch: 3 [25600/32000 (80%)]\tLoss: 0.185891\nTest set: Average loss: 0.0000\n"
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    ppo_student,\n",
    "    epochs=3,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    ")\n",
    "ppo_student.save(\"a2c_student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-dd1e078b8963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean reward = {mean_reward} +/- {std_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sb3-torch1.6/lib/python3.6/site-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sb3-torch1.6/lib/python3.6/site-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sb3-torch1.6/lib/python3.6/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_student, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}